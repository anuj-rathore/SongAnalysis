{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "from download_songs import BinacaYear, Song\n",
    "import glob\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse(by):\n",
    "    word_list = collections.defaultdict(float)\n",
    "    \n",
    "    for song in by.songs:\n",
    "        tokens = word_tokenize(song.lyrics)\n",
    "        for word in tokens:\n",
    "            word_list[word] += 1\n",
    "    \n",
    "    total_words = sum(word_list.values())\n",
    "    word_list = {k: v/total_words for k, v in word_list.iteritems()}\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaca_year = {}\n",
    "for i in glob.glob(\"./songs/*.p\"):\n",
    "    binaca_year[int(i[-6:-2])] = pickle.load(open(i, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaca_dict = {}\n",
    "for year in binaca_year.keys():\n",
    "    binaca_dict[year] = analyse(binaca_year[year])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for keys in binaca_dict.values():\n",
    "    for i in keys.keys():\n",
    "        vocab.add(i)\n",
    "vocab = list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dict(year_range):\n",
    "    d = collections.defaultdict(float)\n",
    "    for year in year_range:\n",
    "        for k, v in binaca_dict[year].iteritems():\n",
    "            d[k] += v\n",
    "    return d\n",
    "\n",
    "def get_top_n_words(n, year_range):\n",
    "    d = combine_dict(year_range)\n",
    "    m = [[v, k] for k, v in d.iteritems()]\n",
    "    m.sort(reverse=True)\n",
    "    return set(map(lambda i: i[1], m[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = binaca_dict[1954]\n",
    "m = [(v, k) for k, v in d.iteritems()]\n",
    "m.sort(reverse=True)\n",
    "x = range(len(m))\n",
    "plt.bar(x, map(lambda i: i[0], m), 1/1.5, color=\"blue\")\n",
    "plt.xticks(x, map(lambda i: i[1], m))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bhee , mila , maine , jara , koyi , leke , jaago , aaye , bat , magar , sang , chaand , bahar , ban , gagan , haay , diya , naam , allah , nain , are , najar , sun , ichak , kyaa , phul , ruk , kee , kaun , bol , haa , chand , rang , diye , zindagi , bhayo , toh , tumhe , kahe , mora , more , manzil , yaha , yahi , chup , gayo , ram , haye , kyo , hamara , karu , sab , is , in , kuch , hame , sar , khabar , hindustani , jaha , barse , kah , jiya , kiya , jaise , bade , sa , tha ,\n",
      "---------------------------------------------------------------------------------\n",
      "raja , mehbuba , chori , hath , mile , ga , nahin , jay , hay , jaa , jayenge , kisi , maar , mujhko , apne , du , bas , jaana , sanam , milan , ru , aate , jaate , bole , dekha , jate , jata , jaan , chor , bhai , bhar , dono , prem , tujhe , kahi , jana , chaahiye , jhum , piya , gori , he , apna , ma , aao , aai , aap , kahaan , zara , badal , chale , raha , rahe , aha , karo , saath , mil , hoo , shaadi , nam , jivan , mat , kal , kaise , aisa , aise , bina , thi , raat ,\n"
     ]
    }
   ],
   "source": [
    "#see how words changed over time\n",
    "\n",
    "s_v = slice(100, 200)\n",
    "seta = get_top_n_words(s_v, range(1955, 1965))\n",
    "setb = get_top_n_words(s_v, range(1973, 1983))\n",
    "\n",
    "for i in seta:\n",
    "    if i not in setb:\n",
    "        print i, ',',\n",
    "print\n",
    "print '-' * 81\n",
    "\n",
    "for i in setb:\n",
    "    if i not in seta:\n",
    "        print i, ',',\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load GPU Faiss: No module named swigfaiss_gpu\n",
      "Faiss falling back to CPU-only.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/harshil/Apps/faiss/\")\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_data = np.zeros([len(binaca_dict.keys()), len(vocab)]).astype('float32')\n",
    "\n",
    "word_key = dict()\n",
    "for i, val in enumerate(vocab):\n",
    "    word_key[val] = i\n",
    "\n",
    "for year in binaca_year.keys():\n",
    "    for k, v in binaca_dict[year].iteritems():\n",
    "        year_data[year-min(binaca_dict.keys())][word_key[k]] = float(v)\n",
    "\n",
    "year_data = np.ascontiguousarray(year_data.T)\n",
    "\n",
    "k = 10\n",
    "index = faiss.IndexFlatL2(year_data.shape[1])\n",
    "index.add(year_data)\n",
    "D, I = index.search(year_data, k+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G=nx.Graph()\n",
    "G.add_nodes_from(range(len(vocab)))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    for j in I[i]:\n",
    "        if j == i:\n",
    "            continue\n",
    "        G.add_edge(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hindustani\n",
      "desh dir rona radhika bhigi abhi dadi-amma madhuban saval nasha\n"
     ]
    }
   ],
   "source": [
    "def print_i(j):\n",
    "    print vocab[j]\n",
    "    for i in G.neighbors(j):\n",
    "        print vocab[i],\n",
    "\n",
    "print_i(1732)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1732"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index('hindustani')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat = nx.to_numpy_matrix(G)\n",
    "sc = SpectralClustering(2, affinity='precomputed', n_init=100)\n",
    "sc.fit(adj_mat)\n",
    "\n",
    "print('spectral clustering')\n",
    "print(sc.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
